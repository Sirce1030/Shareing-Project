{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch as t\n",
    "from torch.distributions import Normal, Categorical, Bernoulli\n",
    "from torch.distributions import MultivariateNormal as MvNormal\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib widget\n",
    "from ipywidgets import FloatSlider, IntSlider, interact, interact_manual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\newcommand{\\bracket}[3]{\\left#1 #3 \\right#2}\n",
    "\\newcommand{\\b}{\\bracket{(}{)}}\n",
    "\\newcommand{\\Bernoulli}{{\\rm Bernoulli}\\b}\n",
    "\\newcommand{\\Categorical}{{\\rm Categorical}\\b}\n",
    "\\newcommand{\\x}{\\mathbf{x}}\n",
    "\\newcommand{\\X}{\\mathbf{X}}\n",
    "\\newcommand{\\m}{\\boldsymbol{\\mu}}\n",
    "\\newcommand{\\P}{{\\rm P}\\b}\n",
    "\\newcommand{\\dd}[2][]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\S}{\\mathbf{\\Sigma}}\n",
    "\\newcommand{\\Sh}{\\mathbf{\\hat{\\Sigma}}}\n",
    "\\newcommand{\\mh}{\\boldsymbol{\\hat{\\mu}}}\n",
    "\\newcommand{\\N}{\\mathcal{N}\\b}\n",
    "\\newcommand{\\det}{\\bracket{\\lvert}{\\rvert}}\n",
    "\\newcommand{\\sb}{\\bracket{[}{]}}\n",
    "\\newcommand{\\E}{\\mathbb{E}\\sb}\n",
    "\\newcommand{\\Var}{{\\rm Var}\\sb}\n",
    "\\newcommand{\\Cov}{{\\rm Cov}\\sb}\n",
    "\\DeclareMathOperator*{\\argmax}{arg\\,max}\n",
    "\\DeclareMathOperator*{\\argmin}{arg\\,min}\n",
    "\\newcommand{\\ph}{\\hat{p}}\n",
    "\\newcommand{\\at}{\\bracket{.}{\\rvert}}\n",
    "\\newcommand{\\w}{\\mathbf{w}}\n",
    "\\newcommand{\\W}{\\mathbf{W}}\n",
    "\\newcommand{\\W}{\\mathbf{W}}\n",
    "\\newcommand{\\Wh}{\\mathbf{\\hat{W}}}\n",
    "\\newcommand{\\Y}{\\mathbf{Y}}\n",
    "\\newcommand{\\L}{\\mathcal{L}}\n",
    "\\newcommand{\\wh}{\\mathbf{\\hat{w}}}\n",
    "\\newcommand{\\y}{\\mathbf{y}}\n",
    "\\newcommand{\\0}{\\mathbf{0}}\n",
    "\\newcommand{\\I}{\\mathbf{I}}\n",
    "\\newcommand{\\La}{\\mathbf{\\Lambda}}\n",
    "\\newcommand{\\S}{\\mathbf{\\Sigma}}\n",
    "\\newcommand{\\Sprior}{\\S_\\text{prior}}\n",
    "\\newcommand{\\Spost}{\\S_\\text{post}}\n",
    "\\newcommand{\\mprior}{\\m_\\text{prior}}\n",
    "\\newcommand{\\mpost}{\\m_\\text{post}}\n",
    "\\newcommand{\\Xt}{\\tilde{\\X}}\n",
    "\\newcommand{\\yt}{\\tilde{\\y}}\n",
    "\\newcommand{\\p}{\\mathbf{p}}\n",
    "\\newcommand{\\l}{\\boldsymbol{\\ell}}\n",
    "\\DeclareMathOperator{\\softmax}{softmax}\n",
    "\\newcommand{\\z}{\\mathbf{z}}\n",
    "\\newcommand{\\norm}{\\bracket{\\lVert}{\\rVert}}\n",
    "$$\n",
    "\n",
    "<h1> Lecture 5: Unsupervised learning and clustering </h1>\n",
    "\n",
    "<h2> Clustering/unsupervised learning is not classification/supervised learning (1/2) </h2>\n",
    "Clustering is a type of unsupervised learning, which is very, very different from the supervised learning you have seen in my slides so far.\n",
    "\n",
    "In supervised learning, we have a list of input, `x`, output, `y`, pairs as data, and the idea is to learn a function that maps from a new input test point, to a distribution over the corresonding `Y`\n",
    "\n",
    "```\n",
    "#### Supervised learning\n",
    "x : X # Input\n",
    "y : Y # Output\n",
    "    [(X, Y)] -> (X -> Y) \n",
    "    [(X, Y)] -> (X -> Distribution{Y}) \n",
    "```\n",
    "\n",
    "In unsupervised learning/clustering, we only have a list of inputs, and the goal is to compute a (distribution over) a latent variable that somehow summarises the structure of the inputs.\n",
    "\n",
    "```\n",
    "#### Unsupervised learning\n",
    "x : X # Input\n",
    "z : Z # Latent\n",
    "    [X] -> [Z]\n",
    "    [X] -> [Distribution{Z}]\n",
    "```\n",
    "\n",
    "<h2> First example </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "146db9c46de9416c8326b23e15b80d5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = t.cat([\n",
    "    t.randn(50, 2)/5 + t.tensor([1., 0.]),\n",
    "    t.randn(50, 2)/5 + t.tensor([-1., 0.])\n",
    "])\n",
    "\n",
    "Z = t.cat([\n",
    "    t.zeros(50).int(),\n",
    "    t.ones(50).int()\n",
    "])\n",
    "\n",
    "#fig = plot.figure()\n",
    "\n",
    "fig, axs = plt.subplots(ncols=3, figsize=(9,3), sharey=True)\n",
    "axs[0].set_xlim(-2, 2)\n",
    "axs[1].set_xlim(-2, 2)\n",
    "axs[2].set_xlim(-2, 2)\n",
    "axs[0].set_ylim(-2, 2)\n",
    "axs[0].scatter(X[:, 0], X[:, 1])\n",
    "axs[1].scatter(X[:, 0], X[:, 1], c=Z);\n",
    "axs[2].scatter(X[:, 0], X[:, 1], c=-Z);\n",
    "axs[0].set_title(\"original data, without labels\")\n",
    "axs[1].set_title(\"clustered data\")\n",
    "axs[2].set_title(\"equiv. clustering\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Clustering/unsupervised learning is not classification/supervised learning (2/2) </h2>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a80fed6eb51417c90f0eece80cb81a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Y = Bernoulli(t.sigmoid(100*X[:, 1])).sample()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlim(-2, 2)\n",
    "ax.set_ylim(-1, 1)\n",
    "ax.set_xlabel(\"height\")\n",
    "ax.set_ylabel(\"A-level scores\")\n",
    "ax.scatter(X[:, 0], X[:, 1], c=Y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Simplest clustering algorithm: K-means </h2>\n",
    "\n",
    "There are parameters, $\\m$, and latent variables, $z_\\lambda$, associated with each datapoint.\n",
    "\n",
    "The parameters, $\\m$ describe the central points of $K$ clusters, and the latents assign each datapoint to a cluster.  Thus, we have three/four key objects,\n",
    "```\n",
    "X.shape  = (N, 1, D)\n",
    "mu.shape =    (K, D)\n",
    "p.shape  = (N, K, 1)\n",
    "z.shape  = (N,)\n",
    "```\n",
    "where `N` is the number of datapoints, `D` is the dimension of each datapoint, and `p` is the one-hot representation of `z`.\n",
    "\n",
    "The objective is to minimise the squared distance between each datapoint and its assigned cluster center,\n",
    "\n",
    "\\begin{align}\n",
    "  \\L(\\z, \\m) &= \\sum_\\lambda \\norm{\\x_\\lambda - \\m_{z_\\lambda}}^2\n",
    "\\end{align}\n",
    "\n",
    "To optimize this algorithm, we use coordinate descent (i.e. we alternate between optimizing $\\z$ and $\\m$).\n",
    "\n",
    "First, we assign each datapoint to the nearest cluster,\n",
    "\n",
    "\\begin{align}\n",
    "  z_\\lambda &\\rightarrow \\argmin_k \\norm{\\x_\\lambda - \\m_k}^2\n",
    "\\end{align}\n",
    "\n",
    "Then, we put the cluster centers at the mean of the datapoints,\n",
    "\n",
    "\\begin{align}\n",
    "  \\m_k &\\rightarrow \\frac{1}{\\sum_\\lambda \\delta_{k, z_\\lambda}} \\sum_{\\lambda} \\delta_{k, z_\\lambda} \\x_\\lambda\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ebbcca80c524800946e19e59d9a460b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Button(description='Run Interact', style=ButtonStyle()), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t.manual_seed(0)\n",
    "\n",
    "X = t.cat([\n",
    "    t.randn(50, 2)/5 + t.tensor([1., 0.]),\n",
    "    t.randn(50, 2)/5,\n",
    "    t.randn(50, 2)/5 + t.tensor([-1., 0.])\n",
    "])\n",
    "\n",
    "def kmeans(X, K):\n",
    "    N, D = X.shape\n",
    "    X = X[:, None, :]\n",
    "    mu = t.randn(K, D)\n",
    "    while True:\n",
    "        sd = ((X - mu)**2).sum(-1) # sd.shape = (N, K)\n",
    "        z = t.argmin(sd, 1)        # z.shape  = (N)\n",
    "        q = t.zeros(N, K, 1)\n",
    "        q[t.arange(N), z, 0] = 1.\n",
    "        print(f\"loss = {loss(X, z, mu).item()}\")\n",
    "        plot(X, z, mu)\n",
    "        yield None\n",
    "        \n",
    "        mu = (q * X).sum(0) / q.sum(0)\n",
    "        print(f\"loss = {loss(X, z, mu).item()}\")\n",
    "        plot(X, z, mu)\n",
    "        yield None\n",
    "        \n",
    "def loss(X, z, mu):\n",
    "    return ((X - mu[z, :][:, None, :])**2).mean()\n",
    "    \n",
    "def plot(X, z, mu):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlim(-2, 2)\n",
    "    ax.set_ylim(-2, 2)\n",
    "    ax.scatter(X[:, 0, 0], X[:, 0, 1], c=z);\n",
    "    ax.scatter(mu[:, 0], mu[:, 1], s=100, c='r', label=\"cluster centers\")\n",
    "    ax.legend()\n",
    "    \n",
    "kmeans_iter = iter(kmeans(X, 3))\n",
    "def kmeans_call():\n",
    "    next(kmeans_iter)\n",
    "interact_manual(kmeans_call);\n",
    "\n",
    "#Change marker type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, one strange property of this algorithm is that it assumes \"hard\" cluster assignments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Soft clustering and the Gaussian Mixture Model (GMM) </h2>\n",
    "\n",
    "The goal of the GMM, is really just to model the density of the data we use the latent variables to give us a better model of the data.\n",
    "\n",
    "For instance, consider fitting a MvNormal to the following data,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logPx = -120.26077270507812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/laurence/programs/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:20: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd82305192f54fcc9a6c4f6268043d19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = t.cat([\n",
    "    t.randn(50, 2)/5 + t.tensor([1., 0.]),\n",
    "    t.randn(50, 2)/5 + t.tensor([-1., 0.])\n",
    "])\n",
    "\n",
    "N = X.shape[0]\n",
    "mu = X.sum(0)/N\n",
    "cov = ((X - mu).T @ (X-mu))/N\n",
    "mvn = MvNormal(mu, cov)\n",
    "\n",
    "logPx = mvn.log_prob(X).sum()\n",
    "print(f\"logPx = {logPx}\")\n",
    "\n",
    "P = 100\n",
    "x0s = t.linspace(-2, 2, P)[:, None].expand(P, P)\n",
    "x1s = x0s.T\n",
    "xs  = t.stack([x0s.reshape(-1), x1s.reshape(-1)], 1)\n",
    "ps = mvn.log_prob(xs).exp().view(P, P)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlim(-2, 2)\n",
    "ax.set_ylim(-2, 2)\n",
    "ax.scatter(X[:, 0], X[:, 1]);\n",
    "ax.contour(x0s, x1s, ps);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we get a better model for this kind of data?  We use a GMM.\n",
    "\n",
    "The simplest form of a GMM is a density, with $K$ different Gaussians, in different locations, $\\m_k$, and with different covariance matrices, $\\S_k$,\n",
    "\n",
    "\\begin{align}\n",
    "  \\P{x_\\lambda} &= \\sum_k p_k \\N{\\x_\\lambda; \\m_k, \\S_k}.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this doesn't give us a way to extract any notion of assignment of a datapoint to a mixture component.\n",
    "\n",
    "We therefore write this distribution in terms of a latent variable,\n",
    "\n",
    "\\begin{align}\n",
    "  \\P{\\x_\\lambda} &= \\sum_{z_\\lambda} \\P{\\x_\\lambda| z_\\lambda} \\P{z_\\lambda}\n",
    "\\end{align}\n",
    "\n",
    "where, $\\P{z_\\lambda}$ is a Categorical, and represents the mixture component for the $\\lambda$th data point, and $\\P{x_\\lambda| z_\\lambda}$ is a _single_ Gaussian, (corresponding to the $z_\\lambda$th mixture component),\n",
    "\n",
    "\\begin{align}\n",
    "  \\P{z_\\lambda} &= \\Categorical{z_\\lambda; \\p}\\\\\n",
    "  \\P{\\x_\\lambda| z_\\lambda} &= \\N{\\x_\\lambda; \\m_{z_\\lambda}, \\S_{z_\\lambda}}.\n",
    "\\end{align}\n",
    "\n",
    "The substituting in the values of $\\P{z_\\lambda}$ and $\\P{x_\\lambda| z_\\lambda}$, we get the same expression as above, except use $z_\\lambda$ as the summation index, instead of $k$\n",
    "\n",
    "\\begin{align}\n",
    "  \\P{\\x_\\lambda} &= \\sum_{z_\\lambda} p_{z_\\lambda} \\N{\\x_\\lambda; \\m_{z_\\lambda}, \\S_{z_\\lambda}}.\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use Bayes theorem to give a posterior distribution over $z_\\lambda$,\n",
    "\n",
    "\\begin{align}\n",
    "  \\P{z_\\lambda| \\x_\\lambda} &\\propto \\P{\\x_\\lambda| z_\\lambda} \\P{z_\\lambda}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suggests an algorithm similar to K-means: first compute the posterior over $z_\\lambda$, then update the parameters of the Gaussians, $\\m_k$ and $\\S_k$, using a mean weighted by the posterior.\n",
    "\n",
    "This algorith is known as expectation-maximization (EM).  Computing the posterior is known as the E-step, and updating the parameters is the M-step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d15e16f7a58b4151bd9d94265c0e3c8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Button(description='Run Interact', style=ButtonStyle()), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t.manual_seed(0)\n",
    "\n",
    "X = t.cat([\n",
    "    t.randn(50, 2)/3 + t.tensor([1., 0.]),\n",
    "    t.randn(50, 2)/3 + t.tensor([-1., 0.])\n",
    "])\n",
    "\n",
    "def gmm_em(X, K):\n",
    "    N, D = X.shape\n",
    "    X = X[:, None, :]\n",
    "    mu  = t.randn(K, D)\n",
    "    cov = 0.5*t.eye(D, D).expand(K, -1, -1)\n",
    "    while True:\n",
    "        # unnormalized posterior probability\n",
    "        q = MvNormal(mu, cov).log_prob(X).exp()\n",
    "        # normalized posterior probability\n",
    "        q = q / q.sum(1, keepdim=True)\n",
    "        # expand out last such that q.shape = (N, K, 1)\n",
    "        q = q[:, :,  None]\n",
    "        plot_gmm(X, q, mu, cov)\n",
    "        yield None\n",
    "        \n",
    "        #weighted mean\n",
    "        mu = (q * X).sum(0) / q.sum(0)\n",
    "        plot_gmm(X, q, mu, cov)\n",
    "        yield None\n",
    "\n",
    "        \n",
    "def plot_gmm(X, q, mu, cov):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlim(-2, 2)\n",
    "    ax.set_ylim(-2, 2)\n",
    "    ax.scatter(X[:, 0, 0], X[:, 0, 1], c=q[:, 0, 0]);\n",
    "    ax.scatter(mu[:, 0], mu[:, 1], s=100, c='r', label=\"cluster centers\")\n",
    "    ax.legend()\n",
    "\n",
    "    \n",
    "gmm_em_iter = iter(gmm_em(X, 2))\n",
    "def gmm_em_call():\n",
    "    next(gmm_em_iter)\n",
    "interact_manual(gmm_em_call);\n",
    "\n",
    "#Change marker type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what is the objective function in EM?\n",
    "\n",
    "Turns out that this is a very subtle question.\n",
    "\n",
    "\\begin{align}\n",
    "  \\L{\\m, \\q} &= \\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
